<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/myhz0606/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/myhz0606/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/myhz0606/images/favicon.ico">
  <link rel="mask-icon" href="/myhz0606/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/myhz0606/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/myhz0606/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"github.com","root":"/myhz0606/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null,"valine":{"enable":true,"appId":null,"appKey":null,"serverURLs":null,"placeholder":"Just go go","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":true,"comment_count":true,"recordIP":false}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1 问题引出 1.1 从最近邻搜索谈起(nearest neighbor search, NN) 最近邻检索：给定一张查询图片q(即query)（或文本），从数据库\(\mathcal{X}\)中找到与之最相近的N张图片（或文本）。在实际的检索场景中，我们会用一条向量来作为图片(或文本)的表征。 用公式表达最近邻检索： \[ \mathrm{NN}(q)&#x3D;\mathrm{arg}\mathop{m">
<meta property="og:type" content="article">
<meta property="og:title" content="Image Hash Roadmap">
<meta property="og:url" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/index.html">
<meta property="og:site_name" content="莫叶何竹">
<meta property="og:description" content="1 问题引出 1.1 从最近邻搜索谈起(nearest neighbor search, NN) 最近邻检索：给定一张查询图片q(即query)（或文本），从数据库\(\mathcal{X}\)中找到与之最相近的N张图片（或文本）。在实际的检索场景中，我们会用一条向量来作为图片(或文本)的表征。 用公式表达最近邻检索： \[ \mathrm{NN}(q)&#x3D;\mathrm{arg}\mathop{m">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/HashNet_1.jpeg">
<meta property="og:image" content="https://github.com/images/image_hash_exp/HashNet_1.jpeg">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/HashNet_1.jpeg">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/arch.png">
<meta property="og:image" content="https://github.com/images/image_hash_exp/arch.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/arch.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/DBhash_1.png">
<meta property="og:image" content="https://github.com/images/image_hash_exp/DBhash_1.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/DBhash_1.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/GrayTestsetSearchResult.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/GrayTestsetSearchResult.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/GrayTestsetSearchResult-4894374.png">
<meta property="og:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/GrayTestsetSearchResult-4894374.png">
<meta property="article:published_time" content="2023-01-28T08:05:00.000Z">
<meta property="article:modified_time" content="2023-03-14T06:02:37.034Z">
<meta property="article:author" content="wwjiang">
<meta property="article:tag" content="image retrieval">
<meta property="article:tag" content="图像检索">
<meta property="article:tag" content="哈希检索">
<meta property="article:tag" content="图像哈希">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/myhz0606/2023/01/28/image_hash_exp/images/image_hash_exp/HashNet_1.jpeg">

<link rel="canonical" href="https://github.com/myhz0606/2023/01/28/image_hash_exp/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Image Hash Roadmap | 莫叶何竹</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/myhz0606/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">莫叶何竹</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/myhz0606/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/myhz0606/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/myhz0606/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/myhz0606/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/myhz0606" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://github.com/myhz0606/2023/01/28/image_hash_exp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/myhz0606/images/avatar.gif">
      <meta itemprop="name" content="wwjiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="莫叶何竹">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Image Hash Roadmap
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-28 16:05:00" itemprop="dateCreated datePublished" datetime="2023-01-28T16:05:00+08:00">2023-01-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-03-14 14:02:37" itemprop="dateModified" datetime="2023-03-14T14:02:37+08:00">2023-03-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/myhz0606/categories/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/" itemprop="url" rel="index"><span itemprop="name">图像检索</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="问题引出">1 问题引出</h2>
<h3 id="从最近邻搜索谈起nearest-neighbor-search-nn">1.1
从最近邻搜索谈起(nearest neighbor search, NN)</h3>
<p>最近邻检索：给定一张查询图片q(即query)（或文本），从数据库<span class="math inline">\(\mathcal{X}\)</span>中找到与之最相近的N张图片（或文本）。在实际的检索场景中，我们会用一条向量来作为图片(或文本)的表征。</p>
<p>用公式表达最近邻检索： <span class="math display">\[
\mathrm{NN}(q)=\mathrm{arg}\mathop{min}\limits_{x\in \mathcal{X} } \
\mathrm{dist}(q, x)
\]</span> dist 是某一种距离计算公式（如欧式距离、余弦距离）</p>
<p>直觉来看，最近邻问题很简单，<strong>只要计算query和库中所有向量的距离，再按照距离的大小排序返回最相近样本的索引即可。</strong>但是当数据规模过大时这就成为了一个问题。假设查询图片的向量维度为256（即<span class="math inline">\(d\in
\mathbb{R}^{256}\)</span>）,数据了类型为float64，一条向量的数据大小为
256 * 64 / 8 =
2KB。此时采用这种暴力搜索的方法进行检索，代价会非常大。</p>
<p>（实验CPU：Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz）</p>
<table>
<thead>
<tr class="header">
<th>数据库规模</th>
<th>距离计算耗时</th>
<th>向量数据库大小</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>一百万</td>
<td><span class="math inline">\(\sim\)</span> 24.13s秒</td>
<td><span class="math inline">\(\sim\)</span> 2GB</td>
</tr>
<tr class="even">
<td>一千万</td>
<td><span class="math inline">\(\sim\)</span> 4分钟</td>
<td><span class="math inline">\(\sim\)</span> 20GB</td>
</tr>
<tr class="odd">
<td>一亿</td>
<td><span class="math inline">\(\sim\)</span> 40分钟</td>
<td><span class="math inline">\(\sim\)</span> 200GB</td>
</tr>
</tbody>
</table>
<p>可见当数据量较大时，无论是从向量存储还是从搜索时延都无法满足实际应用需求。在工程中，当向量维度较低时，我们常用Kd-tree来加速搜索。当数据规模过大是（亿级），我们往往用到近似最近邻搜索技术(approximate
nearest neighbor
search，ANN)，ANN技术的核心技术之一就是向量量化技术(vector quantization,
VQ<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>)，常用的方法有乘积量化（product
quantization，PQ<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>）,哈希等。</p>
<h3 id="图片向量哈希是啥有啥难点">1.2 图片向量哈希是啥，有啥难点</h3>
<h4 id="图片向量哈希是啥">1.2.1 图片向量哈希是啥</h4>
<p>向量哈希是ANN技术中向量量化技术的一种常用方法。其目标是学习一个哈希量化函数将一个浮点型或整型的向量量化为一个哈希向量（仅含有两个值，0/1或-1/1）且尽可能的保证搜索结果能够维持（即
similarity preserving）。此时公式（1）可转化为： <span class="math display">\[
\mathrm{HashNN}(q)=\mathrm{arg}\mathop{min}\limits_{x\in \mathcal{X} } \
\mathrm{HammingDist}(q, x)
\]</span></p>
<p><span class="math display">\[
\mathrm{HammingDist}(x_1, x_2)=\mathrm{sum}( x1 \oplus x2)
\]</span></p>
<p>通过上面可以看出图片哈希检索与经典的NN检索有两个主要的不同点：</p>
<ol type="1">
<li><p>图片向量数据类型不同。哈希向量数域集合只有两个值{-1,
1}，浮点向量的数域集合接近无穷。这个特性能够大大降低向量检索任务的内存消耗。以上文256位的向量为例，数据类型为float64时一条向量占用2KB。但为哈希向量时一条数据类型仅为
256 * 1 / 8 =
32B,降低64倍的存储空间。可见哈希能够大幅度降低磁盘、内存的消耗。</p>
<table>
<thead>
<tr class="header">
<th>数据库规模</th>
<th>float64向量数据库大小</th>
<th>哈希向量数据库大小</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>一百万</td>
<td><span class="math inline">\(\sim\)</span> 2GB</td>
<td><span class="math inline">\(\sim\)</span> 31 MB</td>
</tr>
<tr class="even">
<td>一千万</td>
<td><span class="math inline">\(\sim\)</span> 20GB</td>
<td><span class="math inline">\(\sim\)</span> 310 MB</td>
</tr>
<tr class="odd">
<td>一亿</td>
<td><span class="math inline">\(\sim\)</span> 200GB</td>
<td><span class="math inline">\(\sim\)</span> 3GB</td>
</tr>
</tbody>
</table></li>
<li><p>距离计算公式不同。哈希检索用hamming距离作为其距离计算指标。即对两个向量按位求异或后相加。<strong>相对浮点数欧式距离、余弦距离的计算更快。</strong></p></li>
</ol>
<h4 id="图片向量哈希的主要难点要解决什么问题">1.2.2
图片向量哈希的主要难点（要解决什么问题）</h4>
<p>难点1: <strong>如何解决相似度维持问题（similarity
preserving</strong>）。即如何保证哈希后的检索结果和原向量的检索结果尽可能的一致。
<span class="math display">\[
\mathrm{NN}(q) \simeq \mathrm{HashNN}(q)
\]</span> 难点2:
向量哈希需要将向量从浮点数（或整数）映射到只包含两个值的数域空间（0/1或-1/1），这往往会用到符号函数。<strong>但符号函数不可导，如何用梯度下降进行优化？</strong>（针对用深度学习的哈希方法）。</p>
<h2 id="图片向量哈希方法">2 图片向量哈希方法</h2>
<h3 id="相似度维持的解决方案">2.1 相似度维持的解决方案</h3>
<p>在深度哈希任务中，主要通过<strong>设计特定的损失函数来解决相似度维持问题</strong>。</p>
<h4 id="基于成对标签">2.1.1 基于成对标签</h4>
<p>比较有代表性的是DSH<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>,DSDH<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>,DPSH<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>等。假定网络输入的图片<span class="math inline">\(x_1, x_2\)</span>,经过网络输出的浮点向量为<span class="math inline">\(f_1, f_2\)</span>，
经过哈希层后得到的哈希向量分别为<span class="math inline">\(b_1, b_2,
b_i \in \{-1, 1\}^c\)</span>，c是哈希向量的维度。成对标签规则如下：</p>
<ul>
<li>若<span class="math inline">\(x_1,
x_2\)</span>归属同一个类别（或认为相似），则认为其成对标签<span class="math inline">\(s_{12}=1\)</span></li>
<li>若<span class="math inline">\(x_1,
x_2\)</span>归属同不同类别否则为<span class="math inline">\(s_{12}=0\)</span>。</li>
</ul>
<p>对于任意两个数据点<span class="math inline">\(i,j\)</span>其似然概率定义如下</p>
<p><span class="math display">\[
p(s_{ij} | \mathcal{B}) =
\left \{
\begin{aligned}
&amp; \sigma(\Omega_{ij})   \, &amp; s_{ij} = 1 \\
&amp; 1 - \sigma(\Omega_{ij}) \, &amp; s_{ij} = 0
\end{aligned}
\right.
\]</span> 其中 <span class="math inline">\(\Omega_{ij}=\frac{1}{2}b_i^Tb_j\)</span>，<span class="math inline">\(\sigma(\Omega_{ij})=\frac{1}{1+e^{-\Omega_{ij} }
}\)</span></p>
<p>其目标函数为 <span class="math display">\[
\mathop{min}\limits_{\mathcal{B} }=- \mathop{log}
p(\mathcal{S}|\mathcal{B})=-\sum \limits_{s_{ij} \in \mathcal{S} }
\mathop{log} p(s_{ij}| \mathcal{B})=-\sum \limits_{s_{ij} \in
\mathcal{S} } (s_{ij} \Omega_{ij} - log(1 + e^{\Omega_{ij} }))
\]</span>
<strong>此类方法的扩展有</strong>：引入margin来优化类内方差与类间方差<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>、扩展到三元组<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>、基于类别分布进行加权<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>、亦或是扩展到多标签<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<h4 id="基于语义标签">2.1.2 基于语义标签</h4>
<p>这类方法很多时候和成对标签损失一起用<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>。主要思想是：当我们有图片的标签时，只考虑pairwise信息没有充分利用到标签信息，故增加一个分类的损失来协助训练。
<span class="math display">\[
  \sum_{i=1}^{N}L(y_i, W^Tb_i)
\]</span>
主流的分类损失有3种：1）交叉熵损失；2）浮点向量投影得到的概率向量与哈希向量投影得到的二值向量的KL散度；3）L2损失。</p>
<h4 id="基于相似度一致">2.1.3 基于相似度一致</h4>
<p>假定<span class="math inline">\(f\)</span>输入图片的浮点特征向量，<span class="math inline">\(b\)</span>是输入图片的哈希向量。对于一个batch的浮点向量为<span class="math inline">\([f_1, f_2, ..,, f_N]\)</span>，哈希向量为<span class="math inline">\([b_1, b_2, ...,
b_N]\)</span>浮点向量构成的相似度矩阵<span class="math inline">\(S_f=[[f_1f_1^T, f_1f_2^T, ..., f_1f_N^T]; ...;
[f_Nf_1^T, f_Nf_2^T, ...,
f_Nf_N^T]]\)</span>。哈希向量构成的相似度矩阵<span class="math inline">\(S_b=[[b_1b_1^T, b_1b_2^T, ..., b_1b_N^T]; ...;
[b_Nb_1^T, b_Nb_2^T, ..., b_Nb_N^T]]\)</span></p>
<p>在实际的应用中有直接用MSE来使相似度矩阵最小化；也有利用对比学习思想构造相似度矩阵用交叉语义一致性来优化<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>。也有直接利用Batch本身标签的相似度矩阵<span class="math inline">\(S\)</span>来和哈希形成相似度矩阵<span class="math inline">\(S_b\)</span>进行优化<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>。</p>
<h4 id="基于重建">2.1.4 基于重建</h4>
<p>此类方法一般基于VAE架构。其核心思想是：将浮点向量进行哈希量化后，再用哈希向量进行重建，优化目标是重建后与重建前的feature
map尽可能的接近。</p>
<p>比较有代表性的工作是TBH<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>，相较基础的VAE架构同时引入了图卷积来进一步同步哈希向量和浮点向量的特征。</p>
<h4 id="其它相似度维持损失函数">2.1.5 其它相似度维持损失函数</h4>
<p>如优化检索排序的 SortedNCE<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>，优化哈希码聚类分布的
CSQ<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>，基于优化对比量化损失的MeCoQ<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>等。</p>
<h4 id="优化哈希码一些约束">2.1.6 优化哈希码一些约束</h4>
<p>为了避免过拟合往往会增加一个量化损失，使得生成的哈希向量与原向量量化误差别太大。一版采用L2损失或SmoothL1。
<span class="math display">\[
L_{quan} = \sum_i ^ N{ \parallel b_i - f_i \parallel ^2 }
\]</span>
为了使得生成的哈希向量差异性更大往往会增加一个平衡损失使得生成的哈希向量1/-1的个数差不多。
<span class="math display">\[
L_{balance}=\frac{1}{N} \parallel BB^T - I \parallel ^2
\]</span></p>
<h3 id="符号函数不可导的解决方案">2.2 符号函数不可导的解决方案</h3>
<h4 id="改写梯度更新规则">2.2.1 改写梯度更新规则</h4>
<p>Pytorch提供了自定义梯度更新的接口。有代表性的GreedyHash<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>所用的方法前向过程调用符号函数，反向过程不计算符号函数的梯度。实现的关键是Pytorch中的<code>torch.autograd.Function</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GreedyHashLayer</span>(torch.autograd.Function):</span><br><span class="line"> </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.sign()</span><br><span class="line"> </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="keyword">return</span> grad_output</span><br></pre></td></tr></table></figure>
<h4 id="基于松弛relaxation思想训练中替代符号函数为其它可导函数">2.2.2
基于“松弛”（relaxation）思想训练中替代符号函数为其它可导函数</h4>
<p>此类方法是image
hash最为常用的方法。核心思想是：在训练过程将二值化函数用一个可微的函数替代，在推理过程中再用符号函数进行二值化。</p>
<p><img src="./images/image_hash_exp/HashNet_1.jpeg"> <img src="/images/image_hash_exp/HashNet_1.jpeg">
<!-- <img src="./images/image_hash_exp/HashNet_1.jpeg" width = "300" height = "200" alt="图片名称"/> --></p>
<h2 id="构建专利图片哈希检索系统">3 构建专利图片哈希检索系统</h2>
<h3 id="问题分析及技术选型">3.1 问题分析及技术选型</h3>
<p>目前线上已刷1亿+外观向量，1亿+实用新型向量，4亿+发明专利向量。<strong>现阶段我们所用image2vector的pipeline为</strong>
图像 -&gt; 去白边 -&gt; （提轮廓, shape-only）-&gt; resize -&gt;
特征提取 -&gt;
PCA降维。若采用全量训练的方式更新image2vector模型需要全量重刷已有的向量，代价很大。并且目前自训的模型还达不到Facebook在数十亿规模数据训练的基础模型。</p>
<p>本实验采用的image2hash架构为：固定特征提取模块，在已有pipeline上增加一个哈希层达到image2hash的效果。通过对论文的调研，可以尝试的方向有两大类，一类基于模型，一类基于统计。</p>
<p><img src="./images/image_hash_exp/arch.png"> <img src="/images/image_hash_exp/arch.png">
<!-- <img src="./images/image_hash_exp/arch.png" height = "500" alt="arch"/> --></p>
<h4 id="采用模型训练向量哈希参数">3.1.1 采用模型训练向量哈希参数</h4>
<p>与论文场景不同，此处需要将特征提取模块（backbone）的权重全部冻结，只训练哈希层的权重。主要有三类可行的方法：</p>
<p>1）可用公开数据结合标签语义损失+量化损失+平衡损失等进行训练。</p>
<p>2）用专利数据采用自监督的方法进行训练。</p>
<p>3）亦或者直接用最小化浮点向量和哈希向量的量化误差进行训练。</p>
<h5 id="基于开源数据训练哈希参数">3.1.1.1基于开源数据训练哈希参数</h5>
<p>为了快速验证模型效果，首先在imagenet100上进行验证。复现了几个主流的基于深度学习训练哈希层的方法，并在imagenet100上进行测试。（Dbhash是参考Dbnet思想构建的方法）。从结果上，当模型收敛，几类方法的mAP差距不大，但Dbhash的收敛速度特别快，<strong>故采用Dbhash架构来训练哈希层</strong>。</p>
<table>
<thead>
<tr class="header">
<th>epoch</th>
<th>Dbhash</th>
<th>GreedyHash<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></th>
<th>CSQ<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></th>
<th>DHN<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></th>
<th>DPSH<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td>88.61%</td>
<td>86.82%</td>
<td>68.88%</td>
<td>65.85%</td>
<td>66.18%</td>
</tr>
<tr class="even">
<td>20</td>
<td>90.27%</td>
<td>89.46%</td>
<td>78.02%</td>
<td>74.81%</td>
<td>75.88%</td>
</tr>
<tr class="odd">
<td>30</td>
<td>90.34%</td>
<td>90.00%</td>
<td>82.14%</td>
<td>81.87%</td>
<td>82.50%</td>
</tr>
<tr class="even">
<td>40</td>
<td>90.48%</td>
<td>90.21%</td>
<td>84.64%</td>
<td>85.72%</td>
<td>86.03%</td>
</tr>
<tr class="odd">
<td>50</td>
<td>90.55%</td>
<td>90.27%</td>
<td>85.94%</td>
<td>88.11%</td>
<td>88.09%</td>
</tr>
<tr class="even">
<td>60</td>
<td>90.53%</td>
<td>90.56%</td>
<td>86.86%</td>
<td>89.58%</td>
<td>89.28%</td>
</tr>
<tr class="odd">
<td>70</td>
<td>90.60%</td>
<td>90.60%</td>
<td>87.57%</td>
<td>90.75%</td>
<td>89.89%</td>
</tr>
<tr class="even">
<td>80</td>
<td>90.59%</td>
<td>90.59%</td>
<td>88.20%</td>
<td>91.56%</td>
<td>90.37%</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>DBhash 模型架构</strong></p>
<p>整体架构如下图所示，需要训练的哈希层为Thresh layer。</p>
<p><img src="./images/image_hash_exp/DBhash_1.png"> <img src="/images/image_hash_exp/DBhash_1.png">
<!-- <img src="./images/image_hash_exp/DBhash_1.png"  height = "500" alt="图片名称"/> --></p>
<p><strong>训练目标函数有3个</strong>：语义标签分类损失、KL散度及量化损失。（效果见3.3.1）</p>
<p><strong>训练数据</strong>：imagenet1K</p>
<p><strong>PS：</strong>在实际尝试的过程中发现一个坑。BN层的统计数据(running_mean,
running_var)更新是在每一次训练阶段model.train()后的forward()方法中自动实现的，<strong>而不是</strong>在梯度计算与反向传播中更新optim.step()中完成。采用require_grad=False这个方法无法正常冻结。正确的冻结BN的方式是在模型训练时，把BN单独挑出来，重新设置其状态为eval
(在model.train()之后覆盖training状态）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_bn_eval</span>(<span class="params">m</span>):</span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">&#x27;BatchNorm&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">      m.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">model.apply(set_bn_eval)</span><br></pre></td></tr></table></figure>
<h5 id="直接基于最小化量化误差来训练哈希层">3.1.1.2直接基于最小化量化误差来训练哈希层</h5>
<p>本次POC主要尝试了iteration quantization（ITQ<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>）。这个方法的核心思路是将经过PCA的向量数据集中的数据点映射到一个二进制超立方体的顶点上，使得对应的量化误差最小，从而而已得到对应该数据集优良的二进制编码。其核心思路是：找到最优的旋转投影矩阵来使得量化误差最小。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ITQ</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;training hash code by ITQ method&quot;&quot;&quot;</span></span><br><span class="line">    DEFAULT_DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device=DEFAULT_DEVICE</span>):</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_pipeline</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, </span></span><br><span class="line"><span class="params">            vec_rtpath, </span></span><br><span class="line"><span class="params">            file_type, </span></span><br><span class="line"><span class="params">            pca_file=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">            checkpoints=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">            iterations=<span class="number">1000</span>, </span></span><br><span class="line"><span class="params">            thred=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">            debug=<span class="literal">False</span></span></span><br><span class="line"><span class="params">            </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        trainset = self.prepare_patent_data(</span><br><span class="line">            vec_rtpath, file_type, pca_file=pca_file, debug=debug</span><br><span class="line">        )</span><br><span class="line">        self.train(</span><br><span class="line">            trainset, checkpoints=checkpoints, iterations=iterations, thred=thred</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_patent_data</span>(<span class="params">self, vec_rtpath, file_type, pca_file=<span class="literal">None</span>, debug=<span class="literal">False</span></span>):</span><br><span class="line">        vector_ls, image_ls = VectorFileDecoder(file_type=file_type)(</span><br><span class="line">            vec_rtpath, sample_number=<span class="number">30</span> <span class="keyword">if</span> debug <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> pca_file <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            pca_func = <span class="keyword">lambda</span> x: x </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pca_func = PCA(pca_file)</span><br><span class="line">        vector_ls = [pca_func(np.array(i).reshape(<span class="number">1</span>, -<span class="number">1</span>)).reshape(-<span class="number">1</span>).tolist() <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(vector_ls)]</span><br><span class="line">        vector_arr = np.array(vector_ls)  <span class="comment"># N * M</span></span><br><span class="line">        <span class="keyword">return</span> vector_arr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, trainset: np.ndarray, checkpoints=<span class="literal">None</span>, iterations=<span class="number">1000</span>, thred=<span class="literal">None</span></span>):</span><br><span class="line">        trainset_tensor = torch.from_numpy(trainset.astype(np.float32)).to(self.device)</span><br><span class="line">        V = trainset_tensor</span><br><span class="line">        nbits = trainset_tensor.shape[<span class="number">1</span>]</span><br><span class="line">        R = torch.randn(nbits, nbits).to(self.device) </span><br><span class="line">        torch.nn.init.orthogonal_(R)</span><br><span class="line">        origin_quan_loss = self.frobenius_norm(V.sign(), V)</span><br><span class="line">        <span class="keyword">if</span> thred <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            binary_vec = FaissSearchAPI.img2hash_by_thred(</span><br><span class="line">                trainset, thred, to_int8_compress=<span class="literal">False</span></span><br><span class="line">            ).astype(np.float32)</span><br><span class="line">            logger.info(<span class="string">f&quot;generate binary vector by OT method: binary vec shape: <span class="subst">&#123;binary_vec.shape&#125;</span>&quot;</span>)</span><br><span class="line">            origin_quan_loss_2 = self.frobenius_norm(torch.from_numpy(binary_vec).to(self.device), V)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            origin_quan_loss_2 = <span class="number">0.0000</span></span><br><span class="line"></span><br><span class="line">        best_quant_loss = <span class="number">1e10</span></span><br><span class="line">        best_R = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">            <span class="comment"># step 1: update B</span></span><br><span class="line">            V_tilde = V @ R </span><br><span class="line">            B = V_tilde.sign()</span><br><span class="line">            <span class="comment"># step2: update R </span></span><br><span class="line">            [S, _, S_tilde_transpose] = torch.svd(B.t() @ V)</span><br><span class="line">            R = (S_tilde_transpose.t() @ S.t())</span><br><span class="line"></span><br><span class="line">            quant_loss = self.frobenius_norm(B, V_tilde)</span><br><span class="line">            <span class="keyword">if</span> quant_loss &lt; best_quant_loss:</span><br><span class="line">                best_quant_loss = quant_loss</span><br><span class="line">                best_R = R </span><br><span class="line">            logger.info(<span class="string">f&quot;Iter: <span class="subst">&#123;i:04d&#125;</span> frobenius_norm: <span class="subst">&#123;quant_loss:<span class="number">.4</span>f&#125;</span>, BEST: <span class="subst">&#123;best_quant_loss:<span class="number">.4</span>f&#125;</span>, ORIGIN: <span class="subst">&#123;origin_quan_loss:<span class="number">.4</span>f&#125;</span>, ORI2: <span class="subst">&#123;origin_quan_loss_2:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> checkpoints:</span><br><span class="line">            np.savez(checkpoints, R=best_R.cpu().numpy()) </span><br><span class="line">             </span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">frobenius_norm</span>(<span class="params">B, V_tilde</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;$E = || B - V_tilde ||_&#123;F&#125;^&#123;2&#125;$&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (B - V_tilde).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>().sqrt().item() / B.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4 id="基于统计特征获得向量哈希参数">3.1.2
基于统计特征获得向量哈希参数</h4>
<p>该方法的本质是挖掘浮点向量的数值分布统计特征，以最小化量化损失、平衡损失为目标来构建哈希层。基于最优传输理论（Op
timal Transport, OT）的Bi-HalfNet<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>。论文为什么写了很多，感兴趣可以看原文，这里主要讲怎么做。目前我们有1800w的专利图片向量，其构成的矩阵记为<span class="math inline">\(\mathrm{M}=\{m_i |i=1,2,...,18000000 \}, m_i \in
\mathbb{R}^{256}\)</span>,对于向量检索任务，向量<span class="math inline">\(m_i\)</span>可以看作是对应图片<span class="math inline">\(x_i\)</span>的表征。已知<span class="math inline">\(m_i\)</span>是256维，基于OT理论的哈希方法是将<span class="math inline">\(m_i\)</span>的每一维都当作一个特征。已知<span class="math inline">\(m_i=[m_{i,1},
m_{i,2},...,m_{i,256}]\)</span>。将所有样本第一维的特征汇聚一起可得：<span class="math inline">\(f_1=\{m_{i1}|i=1,2, ..., 18000000
\}\)</span>，如何得到一个划分，将<span class="math inline">\(f_i\)</span>转为二值化，且量化误差最小是我们的目标函数，即
<span class="math display">\[
\mathop{min} \sum \limits_{j} \parallel  f_i^{j} - b_i^{j} \parallel ^ 2
\]</span> <span class="math inline">\(f_i^{j}\)</span>表示在<span class="math inline">\(f_i\)</span>的第<span class="math inline">\(j\)</span>个样本；<span class="math inline">\(b_i^{j}\)</span>表示<span class="math inline">\(f_i^{j}\)</span>的哈希表示。</p>
<p>显然<span class="math inline">\(\mathop{mean}
(f_i)\)</span>是我们所需的解。但仅仅考虑量化误差没有兼顾哈希特征的丰富性，在实践中往往会加入一个平衡误差来使得一条哈希向量两个值的数量尽可能的接近。<span class="math inline">\(f_i\)</span>的中位数是我们所需的解。</p>
<p>综上所述，我们只需分别找到每一维的中位数作为该位置的划分，即可获得最小量化误差、最优平衡的哈希表征。</p>
<h3 id="检索系统搭建">3.2检索系统搭建</h3>
<p>Milvus
1.x并不支持哈希检索。本次POC采用Faiss平台搭建基于哈希的检索系统。（吐槽一下，faiss关于搭建哈希搜索的文档真的非常简略）里面有个大坑是插入哈希code导Faiss引擎是需要将哈希吗转为int8后在输入，随后需要将向量的维度变为
<span class="math inline">\(\mathrm{dim} /8\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bytes2int8</span>(<span class="params">byte_vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    convert 0,1 matrix to uint8</span></span><br><span class="line"><span class="string">    shape: (N, M) -&gt; (N, M // 8)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> byte_vec.ndim == <span class="number">2</span></span><br><span class="line">    mutiple = np.array([<span class="number">2</span>**i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">7</span>, -<span class="number">1</span>, -<span class="number">1</span>)]).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    hash_code = byte_vec</span><br><span class="line">    hash_code_uint8 = np.concatenate(</span><br><span class="line">        [np.<span class="built_in">sum</span>(hash_code[:, i * <span class="number">8</span> : (i + <span class="number">1</span>) * <span class="number">8</span>] * mutiple, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(hash_code.shape[<span class="number">1</span>] // <span class="number">8</span>)],</span><br><span class="line">        axis=<span class="number">1</span></span><br><span class="line">    ).astype(np.uint8)</span><br><span class="line">    <span class="keyword">return</span> hash_code_uint8</span><br></pre></td></tr></table></figure>
<h4 id="索引构建">3.2.1索引构建</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FaissSearchEngin</span>:</span><br><span class="line">    SUPPORT_INDEX_TYPE = <span class="built_in">set</span>(</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;binary_flat_ivf&quot;</span>,</span><br><span class="line">            <span class="string">&quot;binary_flat&quot;</span>,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        index_type,</span></span><br><span class="line"><span class="params">        index_params=<span class="built_in">dict</span>(<span class="params"></span>),</span></span><br><span class="line"><span class="params">        index_rtpath=DEFAULT_INDEX_RTPATH,</span></span><br><span class="line"><span class="params">        vector=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        img_name_ls=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">        reuse=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        logger=logger,</span></span><br><span class="line"><span class="params">        prefix = <span class="string">&#x27;&#x27;</span>, </span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index_type(str): faiss index type</span></span><br><span class="line"><span class="string">            index_params(dict): faiss index create parameters</span></span><br><span class="line"><span class="string">            index_rtpath(str): faiss index rtpath</span></span><br><span class="line"><span class="string">            vector(np.ndarray, None): faiss index database</span></span><br><span class="line"><span class="string">            reuse(bool): always create index, whatever</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.logger = logger</span><br><span class="line">        self.reuse = reuse</span><br><span class="line">        self.index_type =  index_type</span><br><span class="line">        <span class="keyword">assert</span> (</span><br><span class="line">            index_type <span class="keyword">in</span> self.SUPPORT_INDEX_TYPE</span><br><span class="line">        ), <span class="string">f&quot;index type only support <span class="subst">&#123;self.SUPPORT_INDEX_TYPE&#125;</span> now!&quot;</span></span><br><span class="line">        self.vector = vector</span><br><span class="line">        self.img_name_ls = img_name_ls</span><br><span class="line">        <span class="keyword">if</span> self.vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            dim = vector.shape[<span class="number">1</span>] * <span class="number">8</span> <span class="keyword">if</span> <span class="string">&quot;binary&quot;</span> <span class="keyword">in</span> self.index_type <span class="keyword">else</span> vector.shape[<span class="number">1</span>]</span><br><span class="line">            index_params.update(<span class="built_in">dict</span>(dim=dim))</span><br><span class="line">        self.logger.info(<span class="string">f&quot;index params: <span class="subst">&#123;index_params&#125;</span>&quot;</span>)</span><br><span class="line">        self.index_save_rtpath = os.path.join(index_rtpath, index_type)</span><br><span class="line">        self.index_path = os.path.join(</span><br><span class="line">            self.index_save_rtpath,</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>_<span class="subst">&#123;<span class="string">&#x27;_&#x27;</span>.join([<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>_<span class="subst">&#123;v&#125;</span>&#x27;</span> <span class="keyword">for</span> k, v <span class="keyword">in</span> index_params.items()])&#125;</span>.index&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        self.index_params = index_params</span><br><span class="line">        checkdir(self.index_save_rtpath)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        self.check_index()</span><br><span class="line">        self.init_index_func(self.index_type)</span><br><span class="line">        self.index = self.build_index(</span><br><span class="line">            self.vector, index_params=self.index_params, index_path=self.index_path, reuse=self.reuse</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_index</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.reuse <span class="keyword">and</span> self.vector <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> os.path.exists(</span><br><span class="line">                self.index_path</span><br><span class="line">            ), <span class="string">f&quot;there are no index file, you should input [vector] parameters to create it&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert_byte_vector_2_uint8</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_index_func</span>(<span class="params">self, index_type</span>):</span><br><span class="line">        <span class="keyword">if</span> index_type == <span class="string">&quot;binary_flat_ivf&quot;</span>:</span><br><span class="line">            self._index_func = self._binary_flat_ivf</span><br><span class="line">            self.write_index_func = faiss.write_index_binary</span><br><span class="line">            self.read_index_func = faiss.read_index_binary</span><br><span class="line">        <span class="keyword">elif</span> index_type == <span class="string">&quot;binary_flat&quot;</span>:</span><br><span class="line">            self._index_func = self._binary_flat</span><br><span class="line">            self.write_index_func = faiss.write_index_binary</span><br><span class="line">            self.read_index_func = faiss.read_index_binary</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query, search_params</span>):</span><br><span class="line">        D, I = self.index.search(query, **search_params)</span><br><span class="line">        <span class="keyword">return</span> D, I</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_index</span>(<span class="params">self, vector, index_path, index_params=<span class="built_in">dict</span>(<span class="params"></span>), reuse=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;create faiss index&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(index_path) <span class="keyword">and</span> reuse:</span><br><span class="line">            self.logger.info(<span class="string">f&quot;read index from exists file: <span class="subst">&#123;index_path&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> self.read_index_func(index_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            index_info = self._index_func(index_params)</span><br><span class="line">            vector = vector.astype(index_info.vec_dtype)</span><br><span class="line">            self.logger.info(<span class="string">f&quot;vector shape: <span class="subst">&#123;vector.shape&#125;</span>, dtype: <span class="subst">&#123;vector.dtype&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> index_info.need_train:</span><br><span class="line">                self.logger.info(<span class="string">f&quot;start train index, please wait...&quot;</span>)</span><br><span class="line">                _t = time.perf_counter()</span><br><span class="line">                index_info.index_func.train(vector)</span><br><span class="line">                self.logger.info(</span><br><span class="line">                    <span class="string">f&quot;train index finish, time consume: <span class="subst">&#123;time.perf_counter() - _t&#125;</span>s&quot;</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.logger.info(<span class="string">f&quot;current index type need not to train!&quot;</span>)</span><br><span class="line">            index_info.index_func.add(vector)</span><br><span class="line">            self.save_id_map(self.index_path.replace(<span class="string">&quot;.index&quot;</span>, <span class="string">&quot;_id_map.txt&quot;</span>))</span><br><span class="line">            self.logger.info(<span class="string">f&quot;save index file to <span class="subst">&#123;index_path&#125;</span>&quot;</span>)</span><br><span class="line">            self.write_index_func(index_info.index_func, index_path)</span><br><span class="line">            <span class="keyword">return</span> index_info.index_func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_binary_flat</span>(<span class="params">self, params</span>):</span><br><span class="line">        index = faiss.IndexBinaryFlat(params.get(<span class="string">&quot;dim&quot;</span>))</span><br><span class="line">        <span class="keyword">return</span> index_info(index, <span class="literal">False</span>, np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_binary_flat_ivf</span>(<span class="params">self, params</span>):</span><br><span class="line">        quantizer = faiss.IndexBinaryFlat(params.get(<span class="string">&quot;dim&quot;</span>))</span><br><span class="line">        index = faiss.IndexBinaryIVF(quantizer, params.get(<span class="string">&quot;dim&quot;</span>), params.get(<span class="string">&quot;nlist&quot;</span>))</span><br><span class="line">        <span class="keyword">return</span> index_info(index, <span class="literal">True</span>, np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, index, training_vector</span>):</span><br><span class="line">        self.logger.info(<span class="string">f&quot;start train index&quot;</span>)</span><br><span class="line">        _t = time.perf_counter()</span><br><span class="line">        index.train(training_vector)</span><br><span class="line">        self.logger.info(<span class="string">f&quot;train index succeed! time consume: <span class="subst">&#123;time.perf_counter() - _t:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">assert</span> index.is_trained, <span class="string">f&quot;index not train!&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_id_map</span>(<span class="params">self, save_path</span>):</span><br><span class="line">        checkdir(os.path.split(save_path)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>.join(self.img_name_ls))</span><br></pre></td></tr></table></figure>
<h3 id="指标及web-demo搭建">3.3 指标及web demo搭建</h3>
<h4 id="指标评估">3.3.1 指标评估</h4>
<p>分别在灰度测试集和轮廓测试集来测试哈希检索的效果。baseline为SQ8量化
（目前线上的量化方式）</p>
<p>候选数据集：140W 外观专利图片数据</p>
<p>测试集：外观专利灰度测试集、外观专利轮廓测试集</p>
<h5 id="精度评估">3.3.1.1 精度评估</h5>
<ul>
<li>灰度测试集的效果对比</li>
</ul>
<table>
<colgroup>
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 31%">
<col style="width: 25%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>topk</th>
<th>baseline</th>
<th>哈希检索（Dbhash）</th>
<th>哈希检索（ITQ）</th>
<th>哈希检索(OT)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td>31.43%</td>
<td>17.30%</td>
<td>22.91%</td>
<td>25.73%</td>
</tr>
<tr class="even">
<td>100</td>
<td>57.86%</td>
<td>39.95%</td>
<td>48.11%</td>
<td>50.54%</td>
</tr>
<tr class="odd">
<td>500</td>
<td>71.82%</td>
<td>56.28%</td>
<td>64.53%</td>
<td>64.09%</td>
</tr>
<tr class="even">
<td>1000</td>
<td>77.35%</td>
<td>64.00%</td>
<td>70.85%</td>
<td>69.18%</td>
</tr>
<tr class="odd">
<td>3000</td>
<td>83.32%</td>
<td>72.78%</td>
<td>78.75%</td>
<td>76.12%</td>
</tr>
<tr class="even">
<td>5000</td>
<td>86.57%</td>
<td>75.94%</td>
<td>81.47%</td>
<td>78.67%</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>89.73%</td>
<td>81.21%</td>
<td>85.34%</td>
<td>82.53%</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="./images/image_hash_exp/GrayTestsetSearchResult.png">
<!-- <img src="./images/image_hash_exp/GrayTestsetSearchResult.png"  height = "500" alt="graytest"/> --></p>
<ul>
<li>轮廓测试集的效果对比</li>
</ul>
<table>
<colgroup>
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 31%">
<col style="width: 25%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>topk</th>
<th>baseline</th>
<th>哈希检索（Dbhash）</th>
<th>哈希检索（ITQ）</th>
<th>哈希检索(OT)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td>4.97%</td>
<td>4.67%</td>
<td>4.19%</td>
<td>5.51%</td>
</tr>
<tr class="even">
<td>100</td>
<td>12.21%</td>
<td>12.33%</td>
<td>9.16%</td>
<td>14.00%</td>
</tr>
<tr class="odd">
<td>500</td>
<td>21.48%</td>
<td>22.20%</td>
<td>15.74%</td>
<td>26.03%</td>
</tr>
<tr class="even">
<td>1000</td>
<td>27.53%</td>
<td>28.61%</td>
<td>20.59%</td>
<td>33.21%</td>
</tr>
<tr class="odd">
<td>3000</td>
<td>37.64%</td>
<td>38.78%</td>
<td>29.44%</td>
<td>42.85%</td>
</tr>
<tr class="even">
<td>5000</td>
<td>41.17%</td>
<td>43.57%</td>
<td>33.45%</td>
<td>46.92%</td>
</tr>
<tr class="odd">
<td>10000</td>
<td>46.62%</td>
<td>52.00%</td>
<td>38.90%</td>
<td>52.24%</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="./images/image_hash_exp/GrayTestsetSearchResult-4894374.png">
<!-- <img src="./images/image_hash_exp/GrayTestsetSearchResult-4894374.png"  height = "500" alt="graytest"/> --></p>
<p>从实验结果看，采用OT方法进行哈希量化能达到最优的效果。并且该方法实现简单，能够复用已刷的向量。</p>
<h5 id="向量存储评估">3.3.3.2 向量存储评估</h5>
<p>以目前256维向量为例。线上采用SQ8量化相较float32存储占用少4倍。哈希量化存储量化在SQ8基础上还能少8倍。</p>
<h5 id="检索速度评估">3.3.1.3 检索速度评估</h5>
<p>候选集大小：140W</p>
<p>测试数据量： 100条</p>
<table>
<thead>
<tr class="header">
<th>检索类型</th>
<th>平均检索耗时</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SQ8检索</td>
<td>50.11m s</td>
</tr>
<tr class="even">
<td>哈希检索</td>
<td>24.92ms</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>哈希检索较SQ8速度提升50.31%</strong></p>
<h4 id="web-demo">3.3.2 web demo</h4>
<table>
<thead>
<tr class="header">
<th>检索类型</th>
<th>web demo 地址</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>常规的SQ8搜索</td>
<td>http://192.168.18.240:20002/baseline</td>
</tr>
<tr class="even">
<td>哈希搜索</td>
<td>http://192.168.18.240:20002/binary_flat_ivf</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="结论">3.4 结论</h3>
<p>本次POC调研了image
hash的常用算法，并在专利图片检索中进行尝试。并对哈希搜索的精度、速度有了一定的认知。</p>
<p><strong>精度上：</strong></p>
<ul>
<li>灰度测试集在top1k召回上，哈希搜索较向量搜索降低8.17% （77.35% -&gt;
69.18%）。但是哈希检索top3K的召回能达到76.12%。若后续做重排的话，粗筛可用哈希搜索。</li>
<li>轮廓测试集在top1k召回上，哈希搜索较向量搜索提升4.68% (27.53% -&gt;
33.21%)</li>
</ul>
<p><strong>存储上：</strong></p>
<ul>
<li>哈希向量比原浮点型内存占用降低32倍，较SQ8向量内存占用降低8倍。</li>
</ul>
<p><strong>检索速度上：</strong></p>
<ul>
<li>哈希搜索相较SQ8提升50.13%（gallery 140w）</li>
</ul>
<p>后续可以应用的方向目前来看有2个：</p>
<ol type="1">
<li>对于粗精排检索范式，可在粗排阶段用哈希做召回，精排阶段用浮点向量做排序。</li>
<li>精度要求不高的检索场景可用哈希检索。</li>
</ol>
<h2 id="附录图片哈希检索的经典论文">4 附录：图片哈希检索的经典论文</h2>
<h3 id="图片哈希检索经典论文">4.1 图片哈希检索经典论文</h3>
<table>
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 36%">
<col style="width: 29%">
<col style="width: 4%">
<col style="width: 2%">
<col style="width: 2%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th>Learning Paradiam</th>
<th>Loss Function</th>
<th>Benchmark</th>
<th>Publish</th>
<th>Cite</th>
<th>year</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CNNH<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></td>
<td>有监督</td>
<td></td>
<td></td>
<td>AAAI</td>
<td>949</td>
<td>2014</td>
</tr>
<tr class="even">
<td style="text-align: left;">SDH<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></td>
<td>有监督</td>
<td>标签语义损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE,MNIST,ImageNet100</td>
<td>CVPR</td>
<td>1121</td>
<td>2015</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DSH<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></td>
<td>有监督</td>
<td>成对标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>CVPR</td>
<td>774</td>
<td>2016</td>
</tr>
<tr class="even">
<td style="text-align: left;">DPSH<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></td>
<td>有监督</td>
<td>成对标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>IJCAI</td>
<td>607</td>
<td>2016</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DTSH<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></td>
<td>有监督</td>
<td>三元组对损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>-</td>
<td>168</td>
<td>2016</td>
</tr>
<tr class="even">
<td style="text-align: left;">DHN<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></td>
<td>有监督</td>
<td>成对标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE,Flickr</td>
<td>AAAI</td>
<td>563</td>
<td>2016</td>
</tr>
<tr class="odd">
<td style="text-align: left;">HashNet<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></td>
<td>有监督</td>
<td>加权成对标签损失，量化损失</td>
<td>ImageNet100,NUS-WIDE,MS COCO</td>
<td>ICCV</td>
<td>503</td>
<td>2017</td>
</tr>
<tr class="even">
<td style="text-align: left;">DSDH<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a></td>
<td>有监督</td>
<td>成对标签损失，语义标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>NIPS</td>
<td>243</td>
<td>2017</td>
</tr>
<tr class="odd">
<td style="text-align: left;">LCDSH<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></td>
<td>有监督</td>
<td>成对标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>IJCAI</td>
<td>13</td>
<td>2017</td>
</tr>
<tr class="even">
<td style="text-align: left;">DAPH<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a></td>
<td>有监督</td>
<td>成对标签损失，平衡损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE</td>
<td>ACM</td>
<td>105</td>
<td>2017</td>
</tr>
<tr class="odd">
<td style="text-align: left;">GreedyHash<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a></td>
<td>有监督</td>
<td>语义标签损失，量化损失</td>
<td>CIFAR-10,ImageNet100</td>
<td>NIPS</td>
<td>100</td>
<td>2018</td>
</tr>
<tr class="even">
<td style="text-align: left;">DCH<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a></td>
<td>有监督</td>
<td></td>
<td>CIFAR-10,NUS-WIDE,MS COCO</td>
<td>CVPR</td>
<td>258</td>
<td>2018</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DFH<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a></td>
<td>有监督</td>
<td>带margin的成对标签损失，语义标签损失，量化损失</td>
<td>CIFAR-10,ImageNet100</td>
<td>BMVC</td>
<td>15</td>
<td>2019</td>
</tr>
<tr class="even">
<td style="text-align: left;">IDHN<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a></td>
<td>有监督，多标签</td>
<td>成对多标签损失，量化损失</td>
<td>NUS-WIDE,Flickr,VOC2012,IAPRTC12</td>
<td>IEEE</td>
<td>82</td>
<td>2019</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DAGH<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a></td>
<td>有监督</td>
<td>成对标签损失，量化损失，平衡损失</td>
<td>CIFAR-10,NUS-WIDE,Fashion-MNIST</td>
<td>ICCV</td>
<td>43</td>
<td>2019</td>
</tr>
<tr class="even">
<td style="text-align: left;">DSHSD<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></td>
<td>有监督</td>
<td>成对标签L2损失，语义标签损失</td>
<td>CIFAR-10,NUS-WIDE,ImageNet100</td>
<td>IEEE</td>
<td>11</td>
<td>2019</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DistillHash<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></td>
<td>无监督</td>
<td></td>
<td></td>
<td>CVPR</td>
<td>105</td>
<td>2019</td>
</tr>
<tr class="even">
<td style="text-align: left;">SPDAQ<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a></td>
<td>有监督</td>
<td>相似度维持损失，平衡损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE-21,NUS-WIDE-81,MS-COCO</td>
<td>AAAI</td>
<td>12</td>
<td>2019</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DBDH<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a></td>
<td>有监督</td>
<td>成对标签损失，平衡损失</td>
<td>MNIST,CIFAR-10,CIFAR-20,Youtube Faces</td>
<td>-</td>
<td>31</td>
<td>2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">CSQ<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a></td>
<td>有监督</td>
<td>平衡损失，基于聚类的量化损失</td>
<td>UCF101,HMDB51,ImageNet100, MS COCO,NUS-WIDE</td>
<td>CVPR</td>
<td>133</td>
<td>2020</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TBH<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a></td>
<td>无监督</td>
<td>重建损失，差异判别损失</td>
<td>CIFAR-10,NUS-WIDE,MS COCO</td>
<td>CVPR</td>
<td>67</td>
<td>2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">DPAH<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a></td>
<td>有监督，多标签</td>
<td>差异正则损失，类间距离差异损失</td>
<td>NUS-WIDE,MS COCO,ImageNet100</td>
<td>WACV</td>
<td>13</td>
<td>2020</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ICICH<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a></td>
<td>有监督，跨模态</td>
<td>-</td>
<td>WIKI,MIRFlickr,NUS-WIDE</td>
<td>-</td>
<td>12</td>
<td>2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">TSDH<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a></td>
<td>有监督</td>
<td>类内中心损失，相似矩阵一致性损失</td>
<td>CIFAR-10,NUS-WIDE,Flickr25K</td>
<td>TNNLS</td>
<td>73</td>
<td>2020</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DDDH<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a></td>
<td>有监督</td>
<td>成对标签损失，语义标签损失，量化损失</td>
<td>CIFAR-10,NUS-WIDE,MS COCO</td>
<td>-</td>
<td>8</td>
<td>2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">EDSH<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a></td>
<td>有监督，跨模态</td>
<td>-</td>
<td>WIKI,MIRFlickr25K,NUS-WIDE</td>
<td>-</td>
<td>21</td>
<td>2020</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SPLH<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a></td>
<td>有监督</td>
<td>相似度维持损失</td>
<td>CIFAR-10,MINIST,Places205</td>
<td>IEEE</td>
<td>30</td>
<td>2020</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bi-Half Net<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a></td>
<td>无监督</td>
<td>量化损失</td>
<td>CIFAR-10,MS COCO,MNIST,UCF101,HMDB51</td>
<td>AAAI</td>
<td>24</td>
<td>2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CIBHash<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a></td>
<td>无监督，对比</td>
<td>对比损失</td>
<td>CIFAR-10,NUS-WIDE,MS COCO</td>
<td>-</td>
<td>13</td>
<td>2021</td>
</tr>
<tr class="even">
<td style="text-align: left;">CIMON<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a></td>
<td>有监督，对比</td>
<td>平行语义一致性损失，交叉语义一致性损失，对比一致损失</td>
<td>CIFAR-10,NUS-WIDE,Flickr25K</td>
<td>-</td>
<td>14</td>
<td>2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SPQ<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a></td>
<td>有监督，对比</td>
<td>交叉对比损失</td>
<td>CIFAR-10,NUS-WIDE,Flickr25K</td>
<td>ICCV</td>
<td>19</td>
<td>2021</td>
</tr>
<tr class="even">
<td style="text-align: left;">BNNH<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a></td>
<td>有监督</td>
<td>相似度维持损失，激活感知损失</td>
<td>CIFAR-10,MNIST,ImageNet100</td>
<td>-</td>
<td>6</td>
<td>2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DCDH<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a></td>
<td>有监督</td>
<td>成对标签损失，类间差异似然损失，语义标签回归损失</td>
<td>YouTube Faces,FaceScrub,CFW-60K,VGGFace2</td>
<td>PR</td>
<td>13</td>
<td>2021</td>
</tr>
<tr class="even">
<td style="text-align: left;">DATE<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a></td>
<td>无监督</td>
<td>语义维持损失，对比损失</td>
<td>CIFAR-10,Flickr25K,NUSWIDE-10,NUSWIDE-21</td>
<td>MM</td>
<td>7</td>
<td>2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SortedNCE<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a></td>
<td>有监督</td>
<td>排序损失</td>
<td>CIFAR-10,NUS-WIDE,MS COCO</td>
<td>-</td>
<td>3</td>
<td>2022</td>
</tr>
<tr class="even">
<td style="text-align: left;">MeCoQ<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a></td>
<td>无监督</td>
<td>对比量化损失</td>
<td>CIFAR-10,NUS-WIDE,Flickr25K</td>
<td>AAAI</td>
<td>7</td>
<td>2022</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DSPH<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a></td>
<td>有监督，多模态</td>
<td>-</td>
<td>MIRFlickr,NUS-WIDE,Websearch</td>
<td>-</td>
<td>0</td>
<td>2022</td>
</tr>
<tr class="even">
<td style="text-align: left;">Domino<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a></td>
<td>有监督，跨模态</td>
<td>-</td>
<td>CelebA,ImageNet,MIMIC-CXR,EEG</td>
<td>ICLR</td>
<td>27</td>
<td>2022</td>
</tr>
<tr class="odd">
<td style="text-align: left;">VTS<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a></td>
<td>有监督</td>
<td>-</td>
<td>CIFAR-10,NUS-WIDE,MS COCO,ImageNet100</td>
<td>ICME</td>
<td>8</td>
<td>2022</td>
</tr>
<tr class="even">
<td style="text-align: left;">CLIP4hashing<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a></td>
<td>有监督</td>
<td>相似矩阵一致性损失</td>
<td>MSRVTT,DiDeMo,MSVD</td>
<td>ICMR</td>
<td>1</td>
<td>2022</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="reference">Reference</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/1162229/">Vector
quantization</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/5432202/">Product
quantization for nearest neighbor search</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf">Deep
Supervised Hashing for Fast Image Retrieval</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf">Deep
Supervised Discrete Hashing</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.03855.pdf">Feature
Learning based Deep Supervised Hashing with Pairwise Labels</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.00206.pdf">Push for
Quantization: Deep Fisher Hashing</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.03900.pdf">Deep
Supervised Hashing with Triplet Labels</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_HashNet_Deep_Learning_ICCV_2017_paper.pdf">HashNet:
Deep Learning to Hash by Continuation</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02987.pdf">Improved
Deep Hashing with Soft Pairwise Similarity for Multi-label Image
Retrieval</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf">Deep
Supervised Discrete Hashing</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.07804.pdf">CIMON:
Towards High-quality Hash Codes</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a target="_blank" rel="noopener" href="https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng&#39;s%20Homepage_files/Papers/Journal/TNNLS2020_Cheng.pdf">Two-Stream
Deep Hashing With Class-Specific Centers for Supervised Image
Search</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Auto-Encoding_Twin-Bottleneck_Hashing_CVPR_2020_paper.pdf">Auto-Encoding
twin bottleneck hashing</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.13322.pdf">Learning
to Hash Naturally Sorts</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Central_Similarity_Quantization_for_Efficient_Image_and_Video_Retrieval_CVPR_2020_paper.pdf">Central
Similarity Quantization for Efficient Image and Video Retrieval</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/download/20147/19906">Contrastive
Quantization with Code Memory for Unsupervised Image Retrieval</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf">Greedy
Hash: Towards Fast Optimization for Accurate Hash Coding in CNN</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf">Greedy
Hash: Towards Fast Optimization for Accurate Hash Coding in CNN</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Central_Similarity_Quantization_for_Efficient_Image_and_Video_Retrieval_CVPR_2020_paper.pdf">Central
Similarity Quantization for Efficient Image and Video Retrieval</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/10235/10094">Deep
Hashing Network for Efficient Similarity Retrieval</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.03855.pdf">Feature
Learning based Deep Supervised Hashing with Pairwise Labels</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/6296665/">Iterative
quantization: A procrustean approach to learning binary codes for
large-scale image retrieval</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16296/16103">Deep
Unsupervised Image Hashing by Maximizing Bit Entropy</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p><a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Supervised+Hashing++for+Image+Retrieval+via+Image+Representation+Learning&amp;btnG=">Supervised
Hashing for Image Retrieval via Image Representation Learning</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf">Supervised
Discrete Hashing</a><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf">Deep
Supervised Hashing for Fast Image Retrieval</a><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.03855.pdf">Feature
Learning based Deep Supervised Hashing with Pairwise Labels</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.03900.pdf">Deep
Supervised Hashing with Triplet Labels</a><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/10235/10094">Deep
Hashing Network for Efficient Similarity Retrieval</a><a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Cao_HashNet_Deep_Learning_ICCV_2017_paper.pdf">HashNet:
Deep Learning to Hash by Continuation</a><a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2017/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf">Deep
Supervised Discrete Hashing</a><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p><a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2017/0499.pdf">Locality-Constrained
Deep Supervised Hashing for Image Retrieval</a><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p><a target="_blank" rel="noopener" href="https://cfm.uestc.edu.cn/~fshen/DAPH.pdf">Deep
Asymmetric Pairwise Hashing</a><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf">Greedy
Hash: Towards Fast Optimization for Accurate Hash Coding in CNN</a><a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.pdf">Deep
Cauchy Hashing for Hamming Space Retrieval</a><a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.00206.pdf">Push for
Quantization: Deep Fisher Hashing</a><a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02987.pdf">Improved
Deep Hashing with Soft Pairwise Similarity for Multi-label Image
Retrieval</a><a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Supervised_Hashing_With_Anchor_Graph_ICCV_2019_paper.pdf">Deep
Supervised Hashing with Anchor Graph</a><a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8648432/">Deep Supervised
Hashing Based on Stable Distribution</a><a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_DistillHash_Unsupervised_Deep_Hashing_by_Distilling_Data_Pairs_CVPR_2019_paper.pdf">DistillHash:
Unsupervised Deep Hashing by Distilling Data Pairs</a><a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/4828">Similarity
preserving deep asymmetric quantization for image retrieval</a><a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42"><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220306032">Deep
balanced discrete hashing for image retrieval</a><a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Central_Similarity_Quantization_for_Efficient_Image_and_Video_Retrieval_CVPR_2020_paper.pdf">Central
Similarity Quantization for Efficient Image and Video Retrieval</a><a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Auto-Encoding_Twin-Bottleneck_Hashing_CVPR_2020_paper.pdf">Auto-Encoding
twin bottleneck hashing</a><a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Deep_Position-Aware_Hashing_for_Semantic_Continuous_Image_Retrieval_WACV_2020_paper.pdf">Deep
Position-Aware Hashing for Semantic Continuous Image Retrieval</a><a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46"><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0306457319305357">Label
consistent locally linear embedding based cross-modal hashing</a><a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47"><p><a target="_blank" rel="noopener" href="https://see.xidian.edu.cn/faculty/chdeng/Welcome%20to%20Cheng%20Deng&#39;s%20Homepage_files/Papers/Journal/TNNLS2020_Cheng.pdf">Two-Stream
Deep Hashing With Class-Specific Centers for Supervised Image
Search</a><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48"><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0306457320307834">Discriminative
dual-stream deep hashing for large-scale image retrieval</a><a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.01304.pdf">Efficient Discrete
Supervised Hashing for Large-scale Cross-modal Retrieval</a><a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9046296/">Similarity-preserving
linkage hashing for online image retrieval</a><a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16296/16103">Deep
Unsupervised Image Hashing by Maximizing Bit Entropy</a><a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.06138.pdf">Unsupervised Hashing with
Contrastive Information Bottleneck</a><a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.07804.pdf">CIMON:
Towards High-quality Hash Codes</a><a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54"><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jang_Self-Supervised_Product_Quantization_for_Deep_Unsupervised_Image_Retrieval_ICCV_2021_paper.pdf">Self-supervised
Product Quantization for Deep Unsupervised Image Retrieval</a><a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55"><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3404835.3462896">Binary Neural
Network Hashing for Image Retrieval</a><a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56"><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321001631">Deep
center-based dual-constrained hashing for discriminative face image
retrieval</a><a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57"><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3474085.3475570">A Statistical
Approach to Mining Semantic Similarity for Deep Unsupervised
Hashing</a><a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.13322.pdf">Learning
to Hash Naturally Sorts</a><a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn59"><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/download/20147/19906">Contrastive
Quantization with Code Memory for Unsupervised Image Retrieval</a><a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn60"><p><a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Shujuan-Ji/publication/359796301_An_efficient_dual_semantic_preserving_hashing_for_cross-modal_retrieval/links/625680cf328abe6281538210/An-efficient-dual-semantic-preserving-hashing-for-cross-modal-retrieval.pdf">An
efficient dual semantic preserving hashing for cross-modal
retrieval</a><a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.14960">Domino:
Discovering systematic errors with cross-modal embeddings</a><a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn62"><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9859900/">Vision
transformer hashing for image retrieval</a><a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn63"><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3512527.3531381">CLIP4Hashing:
Unsupervised Deep Hashing for Cross-Modal Video-Text Retrieval</a><a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/myhz0606/tags/image-retrieval/" rel="tag"># image retrieval</a>
              <a href="/myhz0606/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/" rel="tag"># 图像检索</a>
              <a href="/myhz0606/tags/%E5%93%88%E5%B8%8C%E6%A3%80%E7%B4%A2/" rel="tag"># 哈希检索</a>
              <a href="/myhz0606/tags/%E5%9B%BE%E5%83%8F%E5%93%88%E5%B8%8C/" rel="tag"># 图像哈希</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/myhz0606/2023/01/28/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item">
    <a href="/myhz0606/2023/02/20/DDPM/" rel="next" title="DDPM(denoising diffusion probabilistic)技术小结">
      DDPM(denoising diffusion probabilistic)技术小结 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%BC%95%E5%87%BA"><span class="nav-number">1.</span> <span class="nav-text">1 问题引出</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2%E8%B0%88%E8%B5%B7nearest-neighbor-search-nn"><span class="nav-number">1.1.</span> <span class="nav-text">1.1
从最近邻搜索谈起(nearest neighbor search, NN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E6%98%AF%E5%95%A5%E6%9C%89%E5%95%A5%E9%9A%BE%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 图片向量哈希是啥，有啥难点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E6%98%AF%E5%95%A5"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 图片向量哈希是啥</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E7%9A%84%E4%B8%BB%E8%A6%81%E9%9A%BE%E7%82%B9%E8%A6%81%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2
图片向量哈希的主要难点（要解决什么问题）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">2 图片向量哈希方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%BB%B4%E6%8C%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 相似度维持的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%88%90%E5%AF%B9%E6%A0%87%E7%AD%BE"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 基于成对标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E6%A0%87%E7%AD%BE"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 基于语义标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%80%E8%87%B4"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 基于相似度一致</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%87%8D%E5%BB%BA"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.4 基于重建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E5%AE%83%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%BB%B4%E6%8C%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.5.</span> <span class="nav-text">2.1.5 其它相似度维持损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%93%88%E5%B8%8C%E7%A0%81%E4%B8%80%E4%BA%9B%E7%BA%A6%E6%9D%9F"><span class="nav-number">2.1.6.</span> <span class="nav-text">2.1.6 优化哈希码一些约束</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%87%BD%E6%95%B0%E4%B8%8D%E5%8F%AF%E5%AF%BC%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 符号函数不可导的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B9%E5%86%99%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 改写梯度更新规则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%9D%BE%E5%BC%9Brelaxation%E6%80%9D%E6%83%B3%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%9B%BF%E4%BB%A3%E7%AC%A6%E5%8F%B7%E5%87%BD%E6%95%B0%E4%B8%BA%E5%85%B6%E5%AE%83%E5%8F%AF%E5%AF%BC%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2
基于“松弛”（relaxation）思想训练中替代符号函数为其它可导函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E4%B8%93%E5%88%A9%E5%9B%BE%E7%89%87%E5%93%88%E5%B8%8C%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F"><span class="nav-number">3.</span> <span class="nav-text">3 构建专利图片哈希检索系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E5%8F%8A%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 问题分析及技术选型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%87%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E5%8F%82%E6%95%B0"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 采用模型训练向量哈希参数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%BC%80%E6%BA%90%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83%E5%93%88%E5%B8%8C%E5%8F%82%E6%95%B0"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">3.1.1.1基于开源数据训练哈希参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E5%8C%96%E9%87%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%AE%AD%E7%BB%83%E5%93%88%E5%B8%8C%E5%B1%82"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">3.1.1.2直接基于最小化量化误差来训练哈希层</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81%E8%8E%B7%E5%BE%97%E5%90%91%E9%87%8F%E5%93%88%E5%B8%8C%E5%8F%82%E6%95%B0"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2
基于统计特征获得向量哈希参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA"><span class="nav-number">3.2.</span> <span class="nav-text">3.2检索系统搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B4%A2%E5%BC%95%E6%9E%84%E5%BB%BA"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1索引构建</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E5%8F%8Aweb-demo%E6%90%AD%E5%BB%BA"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 指标及web demo搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 指标评估</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B2%BE%E5%BA%A6%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">3.3.1.1 精度评估</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">3.3.3.2 向量存储评估</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A3%80%E7%B4%A2%E9%80%9F%E5%BA%A6%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">3.3.1.3 检索速度评估</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#web-demo"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 web demo</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%E5%9B%BE%E7%89%87%E5%93%88%E5%B8%8C%E6%A3%80%E7%B4%A2%E7%9A%84%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87"><span class="nav-number">4.</span> <span class="nav-text">4 附录：图片哈希检索的经典论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%93%88%E5%B8%8C%E6%A3%80%E7%B4%A2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 图片哈希检索经典论文</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">wwjiang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/myhz0606/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/myhz0606/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/myhz0606/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wwjiang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">12k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">45 分钟</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/myhz0606/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/myhz0606/lib/velocity/velocity.min.js"></script>
  <script src="/myhz0606/lib/velocity/velocity.ui.min.js"></script>

<script src="/myhz0606/js/utils.js"></script>

<script src="/myhz0606/js/motion.js"></script>


<script src="/myhz0606/js/schemes/muse.js"></script>


<script src="/myhz0606/js/next-boot.js"></script>

<script src="/myhz0606/js/bookmark.js"></script>




  




  
<script src="/myhz0606/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
